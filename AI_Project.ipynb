{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOt412XResnqc8KhbGf5A+a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JM3927/Hand-ASL-Estimation/blob/main/AI_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cTXQfU34-P6K",
        "outputId": "80588be1-1100-446f-8087-d77088b2bcec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hand-gesture-recognition-mediapipe'...\n",
            "remote: Enumerating objects: 153, done.\u001b[K\n",
            "remote: Counting objects: 100% (70/70), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 153 (delta 50), reused 45 (delta 45), pack-reused 83 (from 1)\u001b[K\n",
            "Receiving objects: 100% (153/153), 1.82 MiB | 8.08 MiB/s, done.\n",
            "Resolving deltas: 100% (82/82), done.\n"
          ]
        }
      ],
      "source": [
        "# https://www.youtube.com/watch?v=a99p_fAr6e4\n",
        "!git clone https://github.com/kinivi/hand-gesture-recognition-mediapipe.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python\n",
        "!pip install mediapipe\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "jxNDpddHBOM3",
        "outputId": "a8794289-9cf7-4b4c-cd80-7bf7463ed50e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.9.23)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n",
            "Collecting numpy<2 (from mediapipe)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.12.0.88)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.2.1)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (0.5.4)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax (from mediapipe)\n",
            "  Downloading jax-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib (from mediapipe)\n",
            "  Downloading jaxlib-0.8.1-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting jax (from mediapipe)\n",
            "  Downloading jax-0.8.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib (from mediapipe)\n",
            "  Downloading jaxlib-0.8.0-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting jax (from mediapipe)\n",
            "  Downloading jax-0.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib (from mediapipe)\n",
            "  Downloading jaxlib-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.12 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "INFO: pip is looking at multiple versions of opencv-contrib-python to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-contrib-python (from mediapipe)\n",
            "  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.3-py3-none-any.whl (32 kB)\n",
            "Downloading jax-0.7.1-py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl (81.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, numpy, sounddevice, opencv-contrib-python, jaxlib, jax, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: opencv-contrib-python\n",
            "    Found existing installation: opencv-contrib-python 4.12.0.88\n",
            "    Uninstalling opencv-contrib-python-4.12.0.88:\n",
            "      Successfully uninstalled opencv-contrib-python-4.12.0.88\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.7.2\n",
            "    Uninstalling jaxlib-0.7.2:\n",
            "      Successfully uninstalled jaxlib-0.7.2\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.7.2\n",
            "    Uninstalling jax-0.7.2:\n",
            "      Successfully uninstalled jax-0.7.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jax-0.7.1 jaxlib-0.7.1 mediapipe-0.10.21 numpy-1.26.4 opencv-contrib-python-4.11.0.86 protobuf-4.25.8 sounddevice-0.5.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2",
                  "google",
                  "numpy"
                ]
              },
              "id": "9ae003e8bad043ac99984648e6a6f5e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.25.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.4)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model from absolute path\n",
        "!wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/latest/hand_landmarker.task"
      ],
      "metadata": {
        "id": "Yn5GDYBxTEhI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Images - TRIM AMOUNT TO 10-20 PER FOLDER"
      ],
      "metadata": {
        "id": "7GPeRRLsYs8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"grassknoted/asl-alphabet\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loRIP_zzFOts",
        "outputId": "4debc3de-a5f7-4742-8c18-61fc196d165d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'asl-alphabet' dataset.\n",
            "Path to dataset files: /kaggle/input/asl-alphabet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Scaling Via Pillow"
      ],
      "metadata": {
        "id": "p78ttPpEmhau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install Pillow\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def resize_image(image_path, output_folder, factor):\n",
        "    try:\n",
        "        with Image.open(image_path) as img:\n",
        "            new_size=(int(img.size[0]*factor),int(img.size[1]* factor))\n",
        "            output_path=os.path.join(output_folder, os.path.basename(image_path))\n",
        "            img = img.resize(new_size)\n",
        "            img.save(output_path)\n",
        "            print(f\"Resized image of {new_size} saved to {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error resizing image{image_path}: {e}\")\n",
        "\n",
        "# def create_output_directory(path):\n",
        "#     if not os.path.exists(path):\n",
        "#         os.makedirs(path)\n",
        "\n",
        "# def get_output_directory(input_directory):\n",
        "#     relative_path = os.path.relpath(input_directory, input_dir)\n",
        "#     return os.path.join(output_dir, relative_path)\n",
        "# for subdir, dirs, files in os.walk(input_dir):\n",
        "#     for file in files:\n",
        "#         if file.endswith(\".JPG\"):\n",
        "#             input_file_path = os.path.join(subdir, file)\n",
        "#             output_directory=get_output_directory(subdir)\n",
        "#             create_output_directory(output_directory)\n",
        "#             output_file_path=os.path.join(output_directory, file)\n",
        "#             if not os.path.exists(output_file_path):\n",
        "#                 resize_image(input_file_path, output_directory, 0.5)\n",
        "\n",
        "def grab_first_20_images(root):\n",
        "    results = []\n",
        "\n",
        "    for mainDir, subDirs, files in os.walk(root):\n",
        "        # Take only the first 20 files in this folder\n",
        "        files = sorted(files)\n",
        "        top20 = files[:4]\n",
        "\n",
        "        # Build full paths\n",
        "        for fname in top20:\n",
        "            results.append(os.path.join(mainDir, fname))\n",
        "\n",
        "    return results\n",
        "\n",
        "ASL_Path = '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train'\n",
        "\n",
        "# Creates unordered list of file paths\n",
        "subdirList = grab_first_20_images(ASL_Path)\n",
        "print(subdirList)\n",
        "\n",
        "resizedImagesPath = \"/content/resized_images\"\n",
        "os.makedirs(resizedImagesPath, exist_ok=True)\n",
        "\n",
        "for path in subdirList:\n",
        "    resize_image(path, resizedImagesPath, 1.28)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YMCb0965Yr-h",
        "outputId": "77abb9ce-67ca-41cd-9e18-dfac35f7ea17"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/N/N1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/N/N10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/N/N100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/N/N1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/R/R1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/R/R10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/R/R100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/R/R1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/space/space1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/space/space10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/space/space100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/space/space1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/B/B1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/B/B10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/B/B100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/B/B1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/I/I1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/I/I10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/I/I100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/I/I1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/del/del1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/del/del10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/del/del100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/del/del1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/F/F1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/F/F10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/F/F100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/F/F1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/H/H1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/H/H10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/H/H100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/H/H1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/E/E1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/E/E10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/E/E100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/E/E1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/U/U1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/U/U10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/U/U100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/U/U1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/M/M1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/M/M10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/M/M100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/M/M1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/X/X1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/X/X10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/X/X100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/X/X1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/K/K1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/K/K10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/K/K100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/K/K1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/Q/Q1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/Q/Q10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/Q/Q100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/Q/Q1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/Y/Y1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/Y/Y10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/Y/Y100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/Y/Y1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/S/S1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/S/S10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/S/S100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/S/S1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/G/G1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/G/G10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/G/G100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/G/G1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/A/A1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/A/A10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/A/A100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/A/A1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/O/O1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/O/O10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/O/O100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/O/O1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/T/T1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/T/T10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/T/T100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/T/T1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/V/V1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/V/V10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/V/V100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/V/V1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/Z/Z1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/Z/Z10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/Z/Z100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/Z/Z1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/C/C1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/C/C10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/C/C100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/C/C1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/P/P1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/P/P10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/P/P100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/P/P1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/L/L1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/L/L10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/L/L100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/L/L1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/W/W1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/W/W10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/W/W100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/W/W1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/D/D1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/D/D10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/D/D100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/D/D1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/nothing/nothing1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/nothing/nothing10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/nothing/nothing100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/nothing/nothing1000.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/J/J1.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/J/J10.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/J/J100.jpg', '/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/J/J1000.jpg']\n",
            "Resized image of (256, 256) saved to /content/resized_images/N1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/N10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/N100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/N1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/R1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/R10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/R100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/R1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/space1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/space10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/space100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/space1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/B1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/B10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/B100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/B1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/I1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/I10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/I100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/I1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/del1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/del10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/del100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/del1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/F1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/F10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/F100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/F1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/H1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/H10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/H100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/H1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/E1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/E10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/E100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/E1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/U1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/U10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/U100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/U1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/M1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/M10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/M100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/M1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/X1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/X10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/X100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/X1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/K1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/K10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/K100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/K1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/Q1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/Q10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/Q100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/Q1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/Y1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/Y10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/Y100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/Y1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/S1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/S10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/S100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/S1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/G1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/G10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/G100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/G1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/A1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/A10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/A100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/A1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/O1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/O10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/O100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/O1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/T1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/T10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/T100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/T1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/V1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/V10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/V100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/V1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/Z1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/Z10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/Z100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/Z1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/C1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/C10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/C100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/C1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/P1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/P10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/P100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/P1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/L1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/L10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/L100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/L1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/W1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/W10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/W100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/W1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/D1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/D10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/D100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/D1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/nothing1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/nothing10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/nothing100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/nothing1000.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/J1.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/J10.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/J100.jpg\n",
            "Resized image of (256, 256) saved to /content/resized_images/J1000.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running Through Base Model for Hand Pose Estimation"
      ],
      "metadata": {
        "id": "SZeH_UxMmr51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import mediapipe as mp\n",
        "\n",
        "# List of letter and the list of coordinates for each image\n",
        "def grab_all_files(root):\n",
        "    results = []\n",
        "\n",
        "    for mainDir, subDirs, files in os.walk(root):\n",
        "        files = sorted(files)\n",
        "\n",
        "        # Build full paths\n",
        "        for fname in files:\n",
        "            results.append(os.path.join(mainDir, fname))\n",
        "\n",
        "    return results\n",
        "\n",
        "imagesPath = '/content/resized_images'\n",
        "fileList = grab_all_files(imagesPath)\n",
        "print(len(fileList))\n",
        "print(fileList)\n",
        "\n",
        "# ex path: /content/resized_images/A1.jpg\n",
        "# Wanted Output: A\n",
        "folderNames = []\n",
        "\n",
        "for path in fileList:\n",
        "    ex = path.split('/')[-1]\n",
        "    ex = ex.split('1')[0]\n",
        "    # List of all group names\n",
        "    if ex not in folderNames:\n",
        "        folderNames.append(ex)\n",
        "\n",
        "# Save data as .csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q50WwXhgmxa7",
        "outputId": "bf625664-89bb-46fa-e95a-64b36914fa93"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "116\n",
            "['/content/resized_images/A1.jpg', '/content/resized_images/A10.jpg', '/content/resized_images/A100.jpg', '/content/resized_images/A1000.jpg', '/content/resized_images/B1.jpg', '/content/resized_images/B10.jpg', '/content/resized_images/B100.jpg', '/content/resized_images/B1000.jpg', '/content/resized_images/C1.jpg', '/content/resized_images/C10.jpg', '/content/resized_images/C100.jpg', '/content/resized_images/C1000.jpg', '/content/resized_images/D1.jpg', '/content/resized_images/D10.jpg', '/content/resized_images/D100.jpg', '/content/resized_images/D1000.jpg', '/content/resized_images/E1.jpg', '/content/resized_images/E10.jpg', '/content/resized_images/E100.jpg', '/content/resized_images/E1000.jpg', '/content/resized_images/F1.jpg', '/content/resized_images/F10.jpg', '/content/resized_images/F100.jpg', '/content/resized_images/F1000.jpg', '/content/resized_images/G1.jpg', '/content/resized_images/G10.jpg', '/content/resized_images/G100.jpg', '/content/resized_images/G1000.jpg', '/content/resized_images/H1.jpg', '/content/resized_images/H10.jpg', '/content/resized_images/H100.jpg', '/content/resized_images/H1000.jpg', '/content/resized_images/I1.jpg', '/content/resized_images/I10.jpg', '/content/resized_images/I100.jpg', '/content/resized_images/I1000.jpg', '/content/resized_images/J1.jpg', '/content/resized_images/J10.jpg', '/content/resized_images/J100.jpg', '/content/resized_images/J1000.jpg', '/content/resized_images/K1.jpg', '/content/resized_images/K10.jpg', '/content/resized_images/K100.jpg', '/content/resized_images/K1000.jpg', '/content/resized_images/L1.jpg', '/content/resized_images/L10.jpg', '/content/resized_images/L100.jpg', '/content/resized_images/L1000.jpg', '/content/resized_images/M1.jpg', '/content/resized_images/M10.jpg', '/content/resized_images/M100.jpg', '/content/resized_images/M1000.jpg', '/content/resized_images/N1.jpg', '/content/resized_images/N10.jpg', '/content/resized_images/N100.jpg', '/content/resized_images/N1000.jpg', '/content/resized_images/O1.jpg', '/content/resized_images/O10.jpg', '/content/resized_images/O100.jpg', '/content/resized_images/O1000.jpg', '/content/resized_images/P1.jpg', '/content/resized_images/P10.jpg', '/content/resized_images/P100.jpg', '/content/resized_images/P1000.jpg', '/content/resized_images/Q1.jpg', '/content/resized_images/Q10.jpg', '/content/resized_images/Q100.jpg', '/content/resized_images/Q1000.jpg', '/content/resized_images/R1.jpg', '/content/resized_images/R10.jpg', '/content/resized_images/R100.jpg', '/content/resized_images/R1000.jpg', '/content/resized_images/S1.jpg', '/content/resized_images/S10.jpg', '/content/resized_images/S100.jpg', '/content/resized_images/S1000.jpg', '/content/resized_images/T1.jpg', '/content/resized_images/T10.jpg', '/content/resized_images/T100.jpg', '/content/resized_images/T1000.jpg', '/content/resized_images/U1.jpg', '/content/resized_images/U10.jpg', '/content/resized_images/U100.jpg', '/content/resized_images/U1000.jpg', '/content/resized_images/V1.jpg', '/content/resized_images/V10.jpg', '/content/resized_images/V100.jpg', '/content/resized_images/V1000.jpg', '/content/resized_images/W1.jpg', '/content/resized_images/W10.jpg', '/content/resized_images/W100.jpg', '/content/resized_images/W1000.jpg', '/content/resized_images/X1.jpg', '/content/resized_images/X10.jpg', '/content/resized_images/X100.jpg', '/content/resized_images/X1000.jpg', '/content/resized_images/Y1.jpg', '/content/resized_images/Y10.jpg', '/content/resized_images/Y100.jpg', '/content/resized_images/Y1000.jpg', '/content/resized_images/Z1.jpg', '/content/resized_images/Z10.jpg', '/content/resized_images/Z100.jpg', '/content/resized_images/Z1000.jpg', '/content/resized_images/del1.jpg', '/content/resized_images/del10.jpg', '/content/resized_images/del100.jpg', '/content/resized_images/del1000.jpg', '/content/resized_images/nothing1.jpg', '/content/resized_images/nothing10.jpg', '/content/resized_images/nothing100.jpg', '/content/resized_images/nothing1000.jpg', '/content/resized_images/space1.jpg', '/content/resized_images/space10.jpg', '/content/resized_images/space100.jpg', '/content/resized_images/space1000.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Passing file list through base model to obtain (mp needs python3.12 or earlier)\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "\n",
        "# Test image\n",
        "# !wget -q -O image.jpg https://storage.googleapis.com/mediapipe-tasks/hand_landmarker/woman_hands.jpg\n",
        "\n",
        "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
        "options = vision.HandLandmarkerOptions(base_options=base_options,\n",
        "                                       num_hands=1)\n",
        "detector = vision.HandLandmarker.create_from_options(options)\n",
        "\n",
        "csvList = [][]\n",
        "newFileList = []\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "numDeleted = 0;\n",
        "\n",
        "# for i in range(len(fileList)):\n",
        "for i in range(1):\n",
        "    imagePath = fileList[i]\n",
        "    image = mp.Image.create_from_file('/content/image.jpg')\n",
        "\n",
        "    detection_result = detector.detect(image)\n",
        "    print(detection_result)\n",
        "\n",
        "    # Iterate through detection_result.hand_landmarks and extract x, y coordinates\n",
        "    if detection_result.hand_landmarks:\n",
        "        newFileList.append(imagePath)\n",
        "        for hand_landmarks in detection_result.hand_landmarks:\n",
        "            csvList[i].append(newFileList[i])\n",
        "            # Append all 21 keypoint values\n",
        "            for landmark in hand_landmarks:\n",
        "                csvList[i].append(landmark.x)\n",
        "                csvList[i].append(landmark.y)\n",
        "    # else:\n",
        "    #     numDeleted += 1\n",
        "\n",
        "\n",
        "\n",
        "print(len(newFileList))\n",
        "print(newFileList)\n",
        "print(len(csvList))\n",
        "print(csvList)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "collapsed": true,
        "id": "ATVcUUxUIBrz",
        "outputId": "b173723c-2bf2-4c26-9d27-bd7267936b03"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2769239237.py, line 15)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2769239237.py\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    csvList = [][]\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running Re-Training for mini NN"
      ],
      "metadata": {
        "id": "sCAWOBqsmyLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "dataset = 'model/keypoint_classifier/keypoint.csv'\n",
        "model_save_path = 'model/keypoint_classifier/keypoint_classifier.keras'\n",
        "tflite_save_path = 'model/keypoint_classifier/keypoint_classifier.tflite'\n",
        "\n",
        "NUM_CLASSES = 4\n",
        "\n",
        "# Normalized on 0, prev. is normalized on input width/height\n",
        "X_dataset = np.loadtxt(dataset, delimiter=',', dtype='float32', usecols=list(range(1, (21 * 2) + 1)))\n",
        "y_dataset = np.loadtxt(dataset, delimiter=',', dtype='int32', usecols=(0))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset, train_size=0.75, random_state=RANDOM_SEED)\n",
        "\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input((21 * 2, )),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(20, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "model.summary()  # tf.keras.utils.plot_model(model, show_shapes=True)\n",
        "\n",
        "# Model checkpoint callback\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    model_save_path, verbose=1, save_weights_only=False)\n",
        "# Callback for early stopping\n",
        "es_callback = tf.keras.callbacks.EarlyStopping(patience=20, verbose=1)\n",
        "# Model compilation\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=1000,\n",
        "    batch_size=128,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[cp_callback, es_callback]\n",
        ")\n",
        "\n",
        "# Model evaluation\n",
        "val_loss, val_acc = model.evaluate(X_test, y_test, batch_size=128)\n",
        "\n",
        "# Loading the saved model\n",
        "model = tf.keras.models.load_model(model_save_path)\n",
        "# Inference test\n",
        "predict_result = model.predict(np.array([X_test[0]]))\n",
        "print(np.squeeze(predict_result))\n",
        "print(np.argmax(np.squeeze(predict_result)))\n",
        "\n",
        "# Save as a model dedicated to inference\n",
        "model.save(model_save_path, include_optimizer=False)\n",
        "# Transform model (quantization)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quantized_model = converter.convert()\n",
        "\n",
        "open(tflite_save_path, 'wb').write(tflite_quantized_model)\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_save_path)\n",
        "interpreter.allocate_tensors()\n",
        "# Get I / O tensor\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "interpreter.set_tensor(input_details[0]['index'], np.array([X_test[0]]))\n",
        "\n",
        "# Inference implementation\n",
        "interpreter.invoke()\n",
        "tflite_results = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print(np.squeeze(tflite_results))\n",
        "print(np.argmax(np.squeeze(tflite_results)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "riLpoerIEIwS",
        "outputId": "0c699296-6c54-47ad-d046-ca135ae3afa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │           \u001b[38;5;34m860\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m210\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │            \u001b[38;5;34m44\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">860</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,114\u001b[0m (4.35 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,114</span> (4.35 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,114\u001b[0m (4.35 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,114</span> (4.35 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "\u001b[1m24/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3419 - loss: 1.3949\n",
            "Epoch 1: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.3449 - loss: 1.3874 - val_accuracy: 0.4261 - val_loss: 1.2495\n",
            "Epoch 2/1000\n",
            "\u001b[1m24/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3917 - loss: 1.2619 \n",
            "Epoch 2: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3933 - loss: 1.2593 - val_accuracy: 0.5079 - val_loss: 1.1595\n",
            "Epoch 3/1000\n",
            "\u001b[1m24/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4226 - loss: 1.1863 \n",
            "Epoch 3: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4232 - loss: 1.1860 - val_accuracy: 0.5789 - val_loss: 1.0829\n",
            "Epoch 4/1000\n",
            "\u001b[1m24/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4732 - loss: 1.1372 \n",
            "Epoch 4: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4750 - loss: 1.1354 - val_accuracy: 0.6617 - val_loss: 1.0149\n",
            "Epoch 5/1000\n",
            "\u001b[1m24/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5093 - loss: 1.0867 \n",
            "Epoch 5: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5098 - loss: 1.0858 - val_accuracy: 0.7135 - val_loss: 0.9535\n",
            "Epoch 6/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5417 - loss: 1.0559 \n",
            "Epoch 6: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5411 - loss: 1.0538 - val_accuracy: 0.7260 - val_loss: 0.9011\n",
            "Epoch 7/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5396 - loss: 1.0287 \n",
            "Epoch 7: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5429 - loss: 1.0261 - val_accuracy: 0.7502 - val_loss: 0.8576\n",
            "Epoch 8/1000\n",
            "\u001b[1m25/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5771 - loss: 0.9778 \n",
            "Epoch 8: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5756 - loss: 0.9778 - val_accuracy: 0.7661 - val_loss: 0.8143\n",
            "Epoch 9/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5670 - loss: 0.9606 \n",
            "Epoch 9: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5687 - loss: 0.9600 - val_accuracy: 0.7686 - val_loss: 0.7672\n",
            "Epoch 10/1000\n",
            "\u001b[1m24/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5874 - loss: 0.9124 \n",
            "Epoch 10: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5888 - loss: 0.9140 - val_accuracy: 0.7920 - val_loss: 0.7170\n",
            "Epoch 11/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5999 - loss: 0.9107 \n",
            "Epoch 11: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6049 - loss: 0.9054 - val_accuracy: 0.8204 - val_loss: 0.6635\n",
            "Epoch 12/1000\n",
            "\u001b[1m24/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6250 - loss: 0.8744 \n",
            "Epoch 12: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6251 - loss: 0.8719 - val_accuracy: 0.8981 - val_loss: 0.6133\n",
            "Epoch 13/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6446 - loss: 0.8222 \n",
            "Epoch 13: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6453 - loss: 0.8236 - val_accuracy: 0.8964 - val_loss: 0.5827\n",
            "Epoch 14/1000\n",
            "\u001b[1m24/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6627 - loss: 0.7931 \n",
            "Epoch 14: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6628 - loss: 0.7941 - val_accuracy: 0.9081 - val_loss: 0.5485\n",
            "Epoch 15/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6720 - loss: 0.7816 \n",
            "Epoch 15: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6704 - loss: 0.7856 - val_accuracy: 0.9031 - val_loss: 0.5262\n",
            "Epoch 16/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6787 - loss: 0.7819 \n",
            "Epoch 16: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6780 - loss: 0.7789 - val_accuracy: 0.9223 - val_loss: 0.5033\n",
            "Epoch 17/1000\n",
            "\u001b[1m18/29\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6739 - loss: 0.7666 \n",
            "Epoch 17: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6755 - loss: 0.7663 - val_accuracy: 0.9298 - val_loss: 0.4821\n",
            "Epoch 18/1000\n",
            "\u001b[1m14/29\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6800 - loss: 0.7500 \n",
            "Epoch 18: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6851 - loss: 0.7458 - val_accuracy: 0.9315 - val_loss: 0.4588\n",
            "Epoch 19/1000\n",
            "\u001b[1m15/29\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6795 - loss: 0.7666 \n",
            "Epoch 19: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6838 - loss: 0.7563 - val_accuracy: 0.9273 - val_loss: 0.4483\n",
            "Epoch 20/1000\n",
            "\u001b[1m17/29\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6937 - loss: 0.7160 \n",
            "Epoch 20: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6912 - loss: 0.7227 - val_accuracy: 0.9290 - val_loss: 0.4420\n",
            "Epoch 21/1000\n",
            "\u001b[1m25/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7122 - loss: 0.7075\n",
            "Epoch 21: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7128 - loss: 0.7077 - val_accuracy: 0.9390 - val_loss: 0.4264\n",
            "Epoch 22/1000\n",
            "\u001b[1m27/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7219 - loss: 0.6977\n",
            "Epoch 22: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7214 - loss: 0.6981 - val_accuracy: 0.9357 - val_loss: 0.4147\n",
            "Epoch 23/1000\n",
            "\u001b[1m26/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7023 - loss: 0.7165\n",
            "Epoch 23: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7035 - loss: 0.7142 - val_accuracy: 0.9373 - val_loss: 0.4026\n",
            "Epoch 24/1000\n",
            "\u001b[1m24/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7052 - loss: 0.6985\n",
            "Epoch 24: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7061 - loss: 0.6976 - val_accuracy: 0.9390 - val_loss: 0.3976\n",
            "Epoch 25/1000\n",
            "\u001b[1m24/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7105 - loss: 0.6832 \n",
            "Epoch 25: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7110 - loss: 0.6843 - val_accuracy: 0.9432 - val_loss: 0.3856\n",
            "Epoch 26/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7285 - loss: 0.6589 \n",
            "Epoch 26: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7273 - loss: 0.6607 - val_accuracy: 0.9449 - val_loss: 0.3852\n",
            "Epoch 27/1000\n",
            "\u001b[1m24/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7262 - loss: 0.6763 \n",
            "Epoch 27: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7278 - loss: 0.6734 - val_accuracy: 0.9415 - val_loss: 0.3727\n",
            "Epoch 28/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7197 - loss: 0.6923 \n",
            "Epoch 28: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7225 - loss: 0.6864 - val_accuracy: 0.9382 - val_loss: 0.3656\n",
            "Epoch 29/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7288 - loss: 0.6667 \n",
            "Epoch 29: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7308 - loss: 0.6650 - val_accuracy: 0.9348 - val_loss: 0.3647\n",
            "Epoch 30/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7400 - loss: 0.6362 \n",
            "Epoch 30: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7418 - loss: 0.6349 - val_accuracy: 0.9457 - val_loss: 0.3465\n",
            "Epoch 31/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7427 - loss: 0.6301 \n",
            "Epoch 31: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7442 - loss: 0.6302 - val_accuracy: 0.9465 - val_loss: 0.3470\n",
            "Epoch 32/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7509 - loss: 0.6227 \n",
            "Epoch 32: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7499 - loss: 0.6246 - val_accuracy: 0.9440 - val_loss: 0.3389\n",
            "Epoch 33/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7388 - loss: 0.6552 \n",
            "Epoch 33: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7422 - loss: 0.6480 - val_accuracy: 0.9457 - val_loss: 0.3267\n",
            "Epoch 34/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7725 - loss: 0.5975 \n",
            "Epoch 34: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7682 - loss: 0.6029 - val_accuracy: 0.9440 - val_loss: 0.3186\n",
            "Epoch 35/1000\n",
            "\u001b[1m24/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7494 - loss: 0.6421 \n",
            "Epoch 35: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7494 - loss: 0.6403 - val_accuracy: 0.9449 - val_loss: 0.3208\n",
            "Epoch 36/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7560 - loss: 0.6218 \n",
            "Epoch 36: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7548 - loss: 0.6234 - val_accuracy: 0.9424 - val_loss: 0.3221\n",
            "Epoch 37/1000\n",
            "\u001b[1m24/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7565 - loss: 0.6186 \n",
            "Epoch 37: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7578 - loss: 0.6168 - val_accuracy: 0.9524 - val_loss: 0.3124\n",
            "Epoch 38/1000\n",
            "\u001b[1m20/29\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7573 - loss: 0.6313 \n",
            "Epoch 38: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7569 - loss: 0.6305 - val_accuracy: 0.9474 - val_loss: 0.3120\n",
            "Epoch 39/1000\n",
            "\u001b[1m24/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7461 - loss: 0.6046 \n",
            "Epoch 39: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7477 - loss: 0.6061 - val_accuracy: 0.9457 - val_loss: 0.3034\n",
            "Epoch 40/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7633 - loss: 0.6127 \n",
            "Epoch 40: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7632 - loss: 0.6126 - val_accuracy: 0.9490 - val_loss: 0.2999\n",
            "Epoch 41/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7677 - loss: 0.5926 \n",
            "Epoch 41: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7679 - loss: 0.5936 - val_accuracy: 0.9482 - val_loss: 0.2974\n",
            "Epoch 42/1000\n",
            "\u001b[1m20/29\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7727 - loss: 0.5937 \n",
            "Epoch 42: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7719 - loss: 0.5945 - val_accuracy: 0.9515 - val_loss: 0.2891\n",
            "Epoch 43/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7634 - loss: 0.6012 \n",
            "Epoch 43: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7649 - loss: 0.5986 - val_accuracy: 0.9457 - val_loss: 0.2881\n",
            "Epoch 44/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7843 - loss: 0.5691 \n",
            "Epoch 44: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7824 - loss: 0.5696 - val_accuracy: 0.9515 - val_loss: 0.2833\n",
            "Epoch 45/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7976 - loss: 0.5591 \n",
            "Epoch 45: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7974 - loss: 0.5589 - val_accuracy: 0.9499 - val_loss: 0.2832\n",
            "Epoch 46/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7758 - loss: 0.5822 \n",
            "Epoch 46: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7732 - loss: 0.5873 - val_accuracy: 0.9499 - val_loss: 0.2864\n",
            "Epoch 47/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7599 - loss: 0.6004 \n",
            "Epoch 47: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7620 - loss: 0.5978 - val_accuracy: 0.9524 - val_loss: 0.2852\n",
            "Epoch 48/1000\n",
            "\u001b[1m24/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7699 - loss: 0.5732 \n",
            "Epoch 48: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7704 - loss: 0.5729 - val_accuracy: 0.9549 - val_loss: 0.2789\n",
            "Epoch 49/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7837 - loss: 0.5754 \n",
            "Epoch 49: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7838 - loss: 0.5725 - val_accuracy: 0.9482 - val_loss: 0.2764\n",
            "Epoch 50/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7883 - loss: 0.5647 \n",
            "Epoch 50: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7855 - loss: 0.5670 - val_accuracy: 0.9482 - val_loss: 0.2792\n",
            "Epoch 51/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7779 - loss: 0.5771 \n",
            "Epoch 51: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7799 - loss: 0.5751 - val_accuracy: 0.9557 - val_loss: 0.2628\n",
            "Epoch 52/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7705 - loss: 0.5679 \n",
            "Epoch 52: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7715 - loss: 0.5690 - val_accuracy: 0.9490 - val_loss: 0.2673\n",
            "Epoch 53/1000\n",
            "\u001b[1m19/29\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7510 - loss: 0.6118 \n",
            "Epoch 53: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7530 - loss: 0.6068 - val_accuracy: 0.9566 - val_loss: 0.2618\n",
            "Epoch 54/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7831 - loss: 0.5616 \n",
            "Epoch 54: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7841 - loss: 0.5610 - val_accuracy: 0.9541 - val_loss: 0.2665\n",
            "Epoch 55/1000\n",
            "\u001b[1m20/29\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7734 - loss: 0.5646 \n",
            "Epoch 55: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7759 - loss: 0.5620 - val_accuracy: 0.9574 - val_loss: 0.2669\n",
            "Epoch 56/1000\n",
            "\u001b[1m18/29\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8046 - loss: 0.5367 \n",
            "Epoch 56: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7991 - loss: 0.5400 - val_accuracy: 0.9574 - val_loss: 0.2618\n",
            "Epoch 57/1000\n",
            "\u001b[1m17/29\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7772 - loss: 0.5557 \n",
            "Epoch 57: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7791 - loss: 0.5581 - val_accuracy: 0.9549 - val_loss: 0.2608\n",
            "Epoch 58/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7955 - loss: 0.5432 \n",
            "Epoch 58: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7947 - loss: 0.5435 - val_accuracy: 0.9532 - val_loss: 0.2569\n",
            "Epoch 59/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7704 - loss: 0.5972 \n",
            "Epoch 59: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7735 - loss: 0.5918 - val_accuracy: 0.9574 - val_loss: 0.2586\n",
            "Epoch 60/1000\n",
            "\u001b[1m21/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7739 - loss: 0.5799 \n",
            "Epoch 60: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7757 - loss: 0.5786 - val_accuracy: 0.9541 - val_loss: 0.2699\n",
            "Epoch 61/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7921 - loss: 0.5371 \n",
            "Epoch 61: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7912 - loss: 0.5387 - val_accuracy: 0.9582 - val_loss: 0.2522\n",
            "Epoch 62/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7879 - loss: 0.5403 \n",
            "Epoch 62: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7870 - loss: 0.5455 - val_accuracy: 0.9557 - val_loss: 0.2530\n",
            "Epoch 63/1000\n",
            "\u001b[1m20/29\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7857 - loss: 0.5516 \n",
            "Epoch 63: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7862 - loss: 0.5536 - val_accuracy: 0.9582 - val_loss: 0.2526\n",
            "Epoch 64/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7963 - loss: 0.5158 \n",
            "Epoch 64: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7968 - loss: 0.5183 - val_accuracy: 0.9574 - val_loss: 0.2465\n",
            "Epoch 65/1000\n",
            "\u001b[1m20/29\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8005 - loss: 0.5354 \n",
            "Epoch 65: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7990 - loss: 0.5369 - val_accuracy: 0.9566 - val_loss: 0.2515\n",
            "Epoch 66/1000\n",
            "\u001b[1m21/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8035 - loss: 0.5173 \n",
            "Epoch 66: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8027 - loss: 0.5230 - val_accuracy: 0.9566 - val_loss: 0.2480\n",
            "Epoch 67/1000\n",
            "\u001b[1m15/29\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8041 - loss: 0.5223 \n",
            "Epoch 67: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7972 - loss: 0.5317 - val_accuracy: 0.9607 - val_loss: 0.2436\n",
            "Epoch 68/1000\n",
            "\u001b[1m18/29\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7785 - loss: 0.5713 \n",
            "Epoch 68: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7809 - loss: 0.5620 - val_accuracy: 0.9582 - val_loss: 0.2447\n",
            "Epoch 69/1000\n",
            "\u001b[1m21/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8009 - loss: 0.5515 \n",
            "Epoch 69: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7995 - loss: 0.5534 - val_accuracy: 0.9582 - val_loss: 0.2491\n",
            "Epoch 70/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7896 - loss: 0.5481 \n",
            "Epoch 70: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7911 - loss: 0.5455 - val_accuracy: 0.9632 - val_loss: 0.2437\n",
            "Epoch 71/1000\n",
            "\u001b[1m21/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7943 - loss: 0.5341\n",
            "Epoch 71: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7940 - loss: 0.5346 - val_accuracy: 0.9607 - val_loss: 0.2384\n",
            "Epoch 72/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7972 - loss: 0.5299\n",
            "Epoch 72: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7998 - loss: 0.5264 - val_accuracy: 0.9591 - val_loss: 0.2341\n",
            "Epoch 73/1000\n",
            "\u001b[1m25/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8254 - loss: 0.4690\n",
            "Epoch 73: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8239 - loss: 0.4750 - val_accuracy: 0.9482 - val_loss: 0.2359\n",
            "Epoch 74/1000\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7950 - loss: 0.5457\n",
            "Epoch 74: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7950 - loss: 0.5455 - val_accuracy: 0.9607 - val_loss: 0.2305\n",
            "Epoch 75/1000\n",
            "\u001b[1m21/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7972 - loss: 0.5246\n",
            "Epoch 75: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7962 - loss: 0.5261 - val_accuracy: 0.9616 - val_loss: 0.2370\n",
            "Epoch 76/1000\n",
            "\u001b[1m20/29\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7987 - loss: 0.5533\n",
            "Epoch 76: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8018 - loss: 0.5430 - val_accuracy: 0.9599 - val_loss: 0.2303\n",
            "Epoch 77/1000\n",
            "\u001b[1m18/29\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7974 - loss: 0.5355  \n",
            "Epoch 77: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7939 - loss: 0.5415 - val_accuracy: 0.9582 - val_loss: 0.2371\n",
            "Epoch 78/1000\n",
            "\u001b[1m20/29\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8001 - loss: 0.5057 \n",
            "Epoch 78: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8005 - loss: 0.5101 - val_accuracy: 0.9582 - val_loss: 0.2346\n",
            "Epoch 79/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8205 - loss: 0.4844 \n",
            "Epoch 79: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8198 - loss: 0.4898 - val_accuracy: 0.9591 - val_loss: 0.2284\n",
            "Epoch 80/1000\n",
            "\u001b[1m19/29\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8139 - loss: 0.4932 \n",
            "Epoch 80: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8139 - loss: 0.4950 - val_accuracy: 0.9574 - val_loss: 0.2249\n",
            "Epoch 81/1000\n",
            "\u001b[1m21/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8014 - loss: 0.5164 \n",
            "Epoch 81: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8037 - loss: 0.5112 - val_accuracy: 0.9591 - val_loss: 0.2253\n",
            "Epoch 82/1000\n",
            "\u001b[1m21/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8060 - loss: 0.4930 \n",
            "Epoch 82: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8045 - loss: 0.5020 - val_accuracy: 0.9599 - val_loss: 0.2288\n",
            "Epoch 83/1000\n",
            "\u001b[1m20/29\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8045 - loss: 0.5376 \n",
            "Epoch 83: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8065 - loss: 0.5315 - val_accuracy: 0.9599 - val_loss: 0.2259\n",
            "Epoch 84/1000\n",
            "\u001b[1m19/29\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7984 - loss: 0.5285 \n",
            "Epoch 84: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7980 - loss: 0.5306 - val_accuracy: 0.9574 - val_loss: 0.2251\n",
            "Epoch 85/1000\n",
            "\u001b[1m21/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8156 - loss: 0.4838 \n",
            "Epoch 85: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8129 - loss: 0.4894 - val_accuracy: 0.9591 - val_loss: 0.2241\n",
            "Epoch 86/1000\n",
            "\u001b[1m20/29\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8028 - loss: 0.5144 \n",
            "Epoch 86: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8027 - loss: 0.5158 - val_accuracy: 0.9599 - val_loss: 0.2208\n",
            "Epoch 87/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7985 - loss: 0.5262 \n",
            "Epoch 87: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7967 - loss: 0.5273 - val_accuracy: 0.9591 - val_loss: 0.2279\n",
            "Epoch 88/1000\n",
            "\u001b[1m21/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8163 - loss: 0.5203 \n",
            "Epoch 88: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8152 - loss: 0.5167 - val_accuracy: 0.9549 - val_loss: 0.2261\n",
            "Epoch 89/1000\n",
            "\u001b[1m20/29\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7923 - loss: 0.5271 \n",
            "Epoch 89: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7927 - loss: 0.5257 - val_accuracy: 0.9574 - val_loss: 0.2267\n",
            "Epoch 90/1000\n",
            "\u001b[1m21/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7959 - loss: 0.5339 \n",
            "Epoch 90: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7971 - loss: 0.5290 - val_accuracy: 0.9582 - val_loss: 0.2208\n",
            "Epoch 91/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8109 - loss: 0.4951 \n",
            "Epoch 91: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8091 - loss: 0.5009 - val_accuracy: 0.9574 - val_loss: 0.2203\n",
            "Epoch 92/1000\n",
            "\u001b[1m21/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8163 - loss: 0.4957 \n",
            "Epoch 92: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8158 - loss: 0.4987 - val_accuracy: 0.9557 - val_loss: 0.2229\n",
            "Epoch 93/1000\n",
            "\u001b[1m20/29\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8127 - loss: 0.4984 \n",
            "Epoch 93: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8113 - loss: 0.5032 - val_accuracy: 0.9582 - val_loss: 0.2236\n",
            "Epoch 94/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8168 - loss: 0.4745 \n",
            "Epoch 94: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8164 - loss: 0.4777 - val_accuracy: 0.9599 - val_loss: 0.2186\n",
            "Epoch 95/1000\n",
            "\u001b[1m20/29\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8101 - loss: 0.5133 \n",
            "Epoch 95: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8114 - loss: 0.5128 - val_accuracy: 0.9591 - val_loss: 0.2180\n",
            "Epoch 96/1000\n",
            "\u001b[1m20/29\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8092 - loss: 0.4950 \n",
            "Epoch 96: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8104 - loss: 0.4951 - val_accuracy: 0.9582 - val_loss: 0.2161\n",
            "Epoch 97/1000\n",
            "\u001b[1m14/29\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7892 - loss: 0.5301 \n",
            "Epoch 97: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7991 - loss: 0.5153 - val_accuracy: 0.9599 - val_loss: 0.2178\n",
            "Epoch 98/1000\n",
            "\u001b[1m21/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8157 - loss: 0.5080 \n",
            "Epoch 98: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8144 - loss: 0.5084 - val_accuracy: 0.9616 - val_loss: 0.2179\n",
            "Epoch 99/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8300 - loss: 0.4697 \n",
            "Epoch 99: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8267 - loss: 0.4756 - val_accuracy: 0.9541 - val_loss: 0.2291\n",
            "Epoch 100/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8318 - loss: 0.4777 \n",
            "Epoch 100: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8301 - loss: 0.4795 - val_accuracy: 0.9607 - val_loss: 0.2162\n",
            "Epoch 101/1000\n",
            "\u001b[1m21/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8310 - loss: 0.4742 \n",
            "Epoch 101: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8278 - loss: 0.4773 - val_accuracy: 0.9591 - val_loss: 0.2173\n",
            "Epoch 102/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8049 - loss: 0.5013 \n",
            "Epoch 102: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8040 - loss: 0.5010 - val_accuracy: 0.9591 - val_loss: 0.2132\n",
            "Epoch 103/1000\n",
            "\u001b[1m21/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8210 - loss: 0.4809 \n",
            "Epoch 103: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8207 - loss: 0.4842 - val_accuracy: 0.9599 - val_loss: 0.2142\n",
            "Epoch 104/1000\n",
            "\u001b[1m21/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8259 - loss: 0.4782 \n",
            "Epoch 104: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8233 - loss: 0.4814 - val_accuracy: 0.9599 - val_loss: 0.2136\n",
            "Epoch 105/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8059 - loss: 0.5230 \n",
            "Epoch 105: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8074 - loss: 0.5183 - val_accuracy: 0.9624 - val_loss: 0.2173\n",
            "Epoch 106/1000\n",
            "\u001b[1m16/29\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8339 - loss: 0.4705 \n",
            "Epoch 106: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8294 - loss: 0.4767 - val_accuracy: 0.9632 - val_loss: 0.2112\n",
            "Epoch 107/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8194 - loss: 0.4915 \n",
            "Epoch 107: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8187 - loss: 0.4931 - val_accuracy: 0.9624 - val_loss: 0.2053\n",
            "Epoch 108/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8238 - loss: 0.4621 \n",
            "Epoch 108: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8233 - loss: 0.4631 - val_accuracy: 0.9641 - val_loss: 0.2028\n",
            "Epoch 109/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8194 - loss: 0.4863 \n",
            "Epoch 109: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8167 - loss: 0.4921 - val_accuracy: 0.9657 - val_loss: 0.2052\n",
            "Epoch 110/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8183 - loss: 0.4759 \n",
            "Epoch 110: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8185 - loss: 0.4775 - val_accuracy: 0.9657 - val_loss: 0.1973\n",
            "Epoch 111/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8187 - loss: 0.4896 \n",
            "Epoch 111: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8187 - loss: 0.4856 - val_accuracy: 0.9599 - val_loss: 0.2110\n",
            "Epoch 112/1000\n",
            "\u001b[1m21/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8269 - loss: 0.4793 \n",
            "Epoch 112: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8228 - loss: 0.4863 - val_accuracy: 0.9566 - val_loss: 0.2177\n",
            "Epoch 113/1000\n",
            "\u001b[1m21/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8257 - loss: 0.4685 \n",
            "Epoch 113: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8244 - loss: 0.4701 - val_accuracy: 0.9649 - val_loss: 0.2032\n",
            "Epoch 114/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8080 - loss: 0.4827 \n",
            "Epoch 114: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8087 - loss: 0.4855 - val_accuracy: 0.9632 - val_loss: 0.2073\n",
            "Epoch 115/1000\n",
            "\u001b[1m21/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8190 - loss: 0.4879 \n",
            "Epoch 115: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8183 - loss: 0.4905 - val_accuracy: 0.9616 - val_loss: 0.2099\n",
            "Epoch 116/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8104 - loss: 0.4925 \n",
            "Epoch 116: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8130 - loss: 0.4894 - val_accuracy: 0.9616 - val_loss: 0.2123\n",
            "Epoch 117/1000\n",
            "\u001b[1m21/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7988 - loss: 0.5296 \n",
            "Epoch 117: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8043 - loss: 0.5216 - val_accuracy: 0.9624 - val_loss: 0.2111\n",
            "Epoch 118/1000\n",
            "\u001b[1m20/29\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8171 - loss: 0.4876 \n",
            "Epoch 118: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8165 - loss: 0.4915 - val_accuracy: 0.9624 - val_loss: 0.2048\n",
            "Epoch 119/1000\n",
            "\u001b[1m22/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8194 - loss: 0.4838 \n",
            "Epoch 119: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8208 - loss: 0.4830 - val_accuracy: 0.9591 - val_loss: 0.2052\n",
            "Epoch 120/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8329 - loss: 0.4657\n",
            "Epoch 120: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8323 - loss: 0.4670 - val_accuracy: 0.9582 - val_loss: 0.2075\n",
            "Epoch 121/1000\n",
            "\u001b[1m25/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8500 - loss: 0.4456\n",
            "Epoch 121: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8461 - loss: 0.4517 - val_accuracy: 0.9591 - val_loss: 0.2122\n",
            "Epoch 122/1000\n",
            "\u001b[1m15/29\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8379 - loss: 0.4515 \n",
            "Epoch 122: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8311 - loss: 0.4630 - val_accuracy: 0.9607 - val_loss: 0.2096\n",
            "Epoch 123/1000\n",
            "\u001b[1m23/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8239 - loss: 0.4718\n",
            "Epoch 123: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8229 - loss: 0.4754 - val_accuracy: 0.9632 - val_loss: 0.2081\n",
            "Epoch 124/1000\n",
            "\u001b[1m25/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8137 - loss: 0.4971\n",
            "Epoch 124: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8153 - loss: 0.4945 - val_accuracy: 0.9632 - val_loss: 0.2006\n",
            "Epoch 125/1000\n",
            "\u001b[1m24/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8284 - loss: 0.4544\n",
            "Epoch 125: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8272 - loss: 0.4570 - val_accuracy: 0.9632 - val_loss: 0.2018\n",
            "Epoch 126/1000\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8398 - loss: 0.4561\n",
            "Epoch 126: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8395 - loss: 0.4566 - val_accuracy: 0.9624 - val_loss: 0.2035\n",
            "Epoch 127/1000\n",
            "\u001b[1m19/29\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8233 - loss: 0.4763 \n",
            "Epoch 127: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8235 - loss: 0.4747 - val_accuracy: 0.9632 - val_loss: 0.2012\n",
            "Epoch 128/1000\n",
            "\u001b[1m20/29\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8290 - loss: 0.4828 \n",
            "Epoch 128: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8280 - loss: 0.4830 - val_accuracy: 0.9624 - val_loss: 0.2037\n",
            "Epoch 129/1000\n",
            "\u001b[1m12/29\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8464 - loss: 0.4628 \n",
            "Epoch 129: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8394 - loss: 0.4630 - val_accuracy: 0.9599 - val_loss: 0.2116\n",
            "Epoch 130/1000\n",
            "\u001b[1m18/29\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8104 - loss: 0.5016 \n",
            "Epoch 130: saving model to model/keypoint_classifier/keypoint_classifier.keras\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8144 - loss: 0.4922 - val_accuracy: 0.9624 - val_loss: 0.2008\n",
            "Epoch 130: early stopping\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9614 - loss: 0.1975 \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "[9.8211265e-01 1.7775605e-02 7.9377271e-05 3.2404831e-05]\n",
            "0\n",
            "Saved artifact at '/tmp/tmpyne1g9pp'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 42), dtype=tf.float32, name='input_layer_4')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134496220514256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134496220520400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134496220527120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134496220528464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134496220515024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134496220515216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "[9.8211265e-01 1.7775603e-02 7.9377271e-05 3.2404860e-05]\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        }
      ]
    }
  ]
}